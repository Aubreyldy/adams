{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAMS Tutorial #4: Neural Networking (NN) Primer\n",
    "## Part 2 of 3\n",
    "This is the second of three notebooks which will cover our initial steps into creating our own neural network. The parts are as follows:\n",
    "\n",
    "- Neural Network Structures and the Forward Pass\n",
    "- Coding a Back Propagation\n",
    "- Changes for Classification\n",
    "\n",
    "## This notebook's topic: back propagation in neural networks ##\n",
    " 1. Overview of the forward pass\n",
    " 2. Caculating loss\n",
    " 2. Finding a minimum\n",
    " 3. Moving towards a minimum\n",
    " 4. One solver for gradient descent: Stochastic gradient descent\n",
    " 5. Stopping Rules\n",
    " 6. Overview of back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(888)  # set seed for reproducibility (numPy functions)\n",
    "random.seed(888)  # set seed for reproducibility (random package functions)\n",
    "\n",
    "range_for_demo = np.linspace(-5, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "n = 1000  # number of observations in our simulation\n",
    "k = 15    # number of features in X in our simulation\n",
    "h = 10    # number of hidden layer nodes\n",
    "\n",
    "X, y = make_regression(n_samples=n, n_features=k, noise=5, random_state=888)\n",
    "\n",
    "y = y.reshape(n, 1)  # Make sure that y is an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we first went in detail through the elements of a neural network's forward pass in Part 2. Please read through that notebook and ensure that you understand its elements. We are now going to build on them for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture:\n",
    "inputLayer_size = k     # number of features in X\n",
    "hiddenLayer_size = h    # just a guess, will need experimentation to optimize\n",
    "outputLayer_size = 1    # number of values to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization for first forward pass\n",
    "\n",
    "limit = np.sqrt(6 / (inputLayer_size + outputLayer_size))  # Recommended weight initialization\n",
    "\n",
    "weightsInputToHidden = np.random.uniform(-limit, limit, (hiddenLayer_size, inputLayer_size))    # Random weight initialization\n",
    "weightsHiddenToOutput = np.random.uniform(-limit, limit, (outputLayer_size, hiddenLayer_size))  # Random weight initialization\n",
    "\n",
    "biasInputToHidden  = np.ones((hiddenLayer_size,1))  # Bias initialization\n",
    "biasHiddenToOutput = np.ones((outputLayer_size,1))  # Bias initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function that we will use in this scenario and its derivative\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)  # 0 if input is negative, x if input is positive\n",
    "\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (ReLU(x) > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First forward pass\n",
    "inputs = np.array(X[5]).reshape((inputLayer_size, 1))  # observation 5\n",
    "target = y[5].reshape((outputLayer_size, 1))  # true value of target\n",
    "hiddenLayer_inputs = np.dot(weightsInputToHidden, inputs) + biasInputToHidden  # first hidden layer inputs\n",
    "hiddenLayer_outputs = ReLU(hiddenLayer_inputs)  # hidden layer output (after activation)\n",
    "outputLayer_inputs = np.dot(weightsHiddenToOutput, hiddenLayer_outputs) + biasHiddenToOutput  # input to output layer\n",
    "outputLayer_outputs = ReLU(outputLayer_inputs)  # activation applied to output layer = prediction!\n",
    "outputLayer_outputs.shape  # shape of first forward pass predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the loss and target for this observation. We can see that the random weight initialization is pretty far off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]] [[207.75432522]]\n"
     ]
    }
   ],
   "source": [
    "print(outputLayer_outputs, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "\n",
    "## Motivation:\n",
    "With a set of random coefficients, would a prediction be any good? Probably not. However, the magic of neural networks comes from the method to update this complex system of weights. This is known as back propagation or backprop. \n",
    "\n",
    "## Calculating loss\n",
    "The first step in back propagation is to calculate the loss (remember: loss is another term for error) which can be done in several ways. For **regression**, we would concentrate on mean square error, mean absolute error or even others such as mean square log error. There are also many other loss functions which act similarly to these. After computing the loss, we then want to minimize it.\n",
    "\n",
    "Note that for some loss functions like the sigmoid for logistic regression, it is best to calculate the log likelihood. You can then derive this for maximization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "def mse(true, pred):\n",
    "    sse = 0\n",
    "    for i in range(len(true)):\n",
    "        sse += (true[i] - pred[i])**2\n",
    "    mse = (1 / len(true)) * np.sum(sse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's calculate the loss from our first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43161.859646772115"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(target, outputLayer_outputs)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a minimum\n",
    "Like a linear regression, we are working with input values along with layers of weights and a biases (= intercepts in the regression). Our loss function is a function of these three elements. Of these three elements, weights and bias are what we can adjust. So, let’s derive our loss function based on those. We will focus on deriving with respect to weights here, but the process for biases is similar.\n",
    "\n",
    "Here is the mean square error. \n",
    "$$ MSE = \\frac{1}{2n} \\sum_{i=1}^n  (\\hat y - y)^2 $$\n",
    "Note that there is a 2 in the initial fraction. This is just for the convenience when we derive it later.\n",
    "\n",
    "<img src=\"MSElossderivation.PNG\" alt=\"Derivative of MSE\" style=\"width: 600px;\"/>\n",
    "\n",
    "note that if $ w_0 $ is the intercept, we would set $ x=1 $.\n",
    "\n",
    "Consider: Our loss function is a function of the true value and our NN’s output. Our NN’s output is a function of the last nodes’ outputs. Our last nodes’ outputs are all a function of their own input values, weights and biases. We’re lucky that we can simply use the chain rule to calculate the derivatives for each parameter! We will then have to organize our derivatives into gradients which are vectors of derivatives of different variables from the same function. For full optimization, we will need to calculate this derivative for every observation as well. We will discuss a simplification technique for this called stochastic gradient descent later.\n",
    "\n",
    "So for each observation, we have to calculate: \n",
    "\n",
    "$$ \\Bigg[ \\frac{\\partial MSE }{\\partial w_0},  \\frac{\\partial MSE }{\\partial w_1}, \\frac{\\partial MSE }{\\partial w_2}, ... ,  \\frac{\\partial MSE }{\\partial w_k} \\Bigg] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_derivative(true, pred, x):\n",
    "    n = len(x)\n",
    "    loss_d = (1/n) * np.sum((pred-true)*x)\n",
    "    return loss_d.reshape(1, 1)\n",
    "\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x > 0)*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-34.98183822]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_derivative = mse_derivative(y[5], outputLayer_outputs, X[5])  # loss derivative\n",
    "loss_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_derivative = ReLU_derivative(outputLayer_inputs)  # activation derivative\n",
    "activation_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the output layer's output is a function of the following:\n",
    "- the output layer's inputs\n",
    "- the hidden layer's outputs\n",
    "- the hidden layer's inputs\n",
    "- the original X values\n",
    "\n",
    "We will need to update all weights connecting these together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first weights (1, 10)\n",
      "Shape of gradient vector (10, 1)\n",
      "Shape of input (15, 1)\n"
     ]
    }
   ],
   "source": [
    "# Update weights furthest back in the network (between hidden and output layer)\n",
    "gradient_HiddenToOutput = np.dot(loss_derivative*activation_derivative, np.transpose(hiddenLayer_outputs))\n",
    "gradient_HiddenToOutput.shape\n",
    "\n",
    "# Update output layer biases\n",
    "gradient_HiddenToOutput_bias = loss_derivative*activation_derivative\n",
    "\n",
    "# Save the error of the output layer\n",
    "outputLayer_errors = loss_derivative * activation_derivative\n",
    "\n",
    "# Find gradient for next step for backpropagation: gradient to update weights between hidden and input layer\n",
    "gradient_InputToHidden = np.dot(weightsHiddenToOutput.T, outputLayer_errors)\n",
    "print(f'Shape of first weights {weightsHiddenToOutput.shape}')\n",
    "\n",
    "# Next propagation backwards: derivative of the hidden layer output wrt the hidden layer input (ReLU derivative)\n",
    "gradient_InputToHidden = gradient_InputToHidden * ReLU_derivative(hiddenLayer_inputs)\n",
    "print(f'Shape of gradient vector {gradient_InputToHidden.shape}')\n",
    "\n",
    "# Last propagation: derivate of the hidden layer input wrt to the weight matrix connecting the hidden layer to inputs X\n",
    "gradient_InputToHidden = np.dot(gradient_InputToHidden, np.transpose(inputs))\n",
    "print(f'Shape of input {inputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving towards a minimum\n",
    "We could update all weights by trying to take a large step down the gradient. However, since there are so many combinations of weights and bias, this just reach one local minimum of many, or it may overshoot the local minimum. It could be that there is a lower local minimum that we cannot yet identify. If we take smaller steps down the gradient, update weights/biases, make predictiongradient_HiddenToOutputs again with these new values and recalculate the gradient, we may find a steeper path which indicates the existence of a lower minimum. This step size is called a learning rate. This slow and iterative way of updating weights is called gradient descent.\n",
    "\n",
    "$$ w_j' = w_j + lr \\cdot \\frac{\\partial MSE }{\\partial w_1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a few graphics. The one on the left shows the example of trying to fit a curve to points. The figure on the right is a depiction of slowly moving down a error surface to find a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
       "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
    "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can come back to our scenario and begin the process of updating our weights based on our learning rate * derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_InputToHidden_bias = np.dot(weightsHiddenToOutput.T, outputLayer_errors) * ReLU_derivative(hiddenLayer_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent step\n",
    "learningRate = 0.00001  # define some learning rate\n",
    "\n",
    "# Update weights between hidden and output layer (furthest back)\n",
    "weightsHiddenToOutput -= learningRate * gradient_HiddenToOutput\n",
    "# Update bias in output layer\n",
    "biasHiddenToOutput -= learningRate * gradient_HiddenToOutput_bias\n",
    "# Update weights between input and hidden layer (furthest forward)\n",
    "weightsInputToHidden -= learningRate * gradient_InputToHidden\n",
    "# Update bias in hidden layer\n",
    "biasInputToHidden -= learningRate * gradient_InputToHidden_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One solver for gradient descent: SGD\n",
    "As you can imagine, the number of computations for this process can be quite large. It requires one derivative per observation per parameter per step! Stochastic gradient descent (SGD) is one way to greatly simplify this task. This method picks samples randomly and only calculates their derivatives for optimization. By the strictest definition, SGD picks one sample per optimization step, but **it is common to optimize with batches of observations**. This could lead to a more stable path to a local minimum. In the code below, we will select a new variable batch_size, which will randomly select samples to optimize parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping rules\n",
    "What if we find the program is going through too many iterations with little improvement? We need to set a stopping rule to decide when the program should end. There are two very commonly chosen options: specifying a maximum number of epochs (= iterations) allowed or specifying a maximum allowed number of epochs without improvement in the accuracy of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.00001\n",
    "epochs = 10  # stopping rule, how many corrective iterations we will allow\n",
    "batch_size = 100\n",
    "\n",
    "input_dim = X.shape[1]  # number of variables\n",
    "output_dim = 1  # should be equal to 1 since we are only finding the predicted value for 1 regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 with average error of 5481.54.\n",
      "Epoch 1 with average error of 5481.23.\n",
      "Epoch 2 with average error of 5480.74.\n",
      "Epoch 3 with average error of 5479.89.\n",
      "Epoch 4 with average error of 5478.98.\n",
      "Epoch 5 with average error of 5477.78.\n",
      "Epoch 6 with average error of 5476.90.\n",
      "Epoch 7 with average error of 5476.01.\n",
      "Epoch 8 with average error of 5475.04.\n",
      "Epoch 9 with average error of 5473.83.\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "while iteration < epochs:\n",
    "\n",
    "    # Put all the steps done before in a loop:\n",
    "    # Process one observation per iteration\n",
    "    squared_residuals = np.zeros(X.shape[0])  # to keep track of the loss\n",
    "\n",
    "    random_obs = random.sample(range(0, len(X)), batch_size)  # randomly select a new sample of dataset for this iteration's optimization\n",
    "\n",
    "    for obs in random_obs:\n",
    "\n",
    "        inputs = np.array(X[obs]).reshape((input_dim, 1))  # select current case i\n",
    "        target = y[obs].reshape((1, 1))  # and its target value\n",
    "\n",
    "        # Compute the forward pass through the network all the way up to the final output\n",
    "        hiddenLayer_inputs = np.dot(weightsInputToHidden, inputs) + biasInputToHidden\n",
    "        hiddenLayer_outputs = ReLU(hiddenLayer_inputs)\n",
    "        outputLayer_inputs = np.dot(weightsHiddenToOutput, hiddenLayer_outputs) + biasHiddenToOutput\n",
    "        outputLayer_outputs = ReLU(outputLayer_inputs)  # final output\n",
    "\n",
    "        # Compute the gradients:\n",
    "        # gradient for the weights between hidden and output layers\n",
    "        gradient_HiddenToOutput = mse_derivative(y[obs], outputLayer_outputs, X[obs]) * ReLU_derivative(outputLayer_outputs)\n",
    "        outputLayer_errors = gradient_HiddenToOutput\n",
    "\n",
    "        # gradient for the weights between input and hidden layers\n",
    "        gradient_InputToHidden = np.dot(weightsHiddenToOutput.T, outputLayer_errors) * ReLU_derivative(hiddenLayer_inputs)\n",
    "\n",
    "        # Perform gradient descent update\n",
    "        biasHiddenToOutput -= learningRate * gradient_HiddenToOutput\n",
    "        biasInputToHidden -= learningRate * gradient_InputToHidden\n",
    "\n",
    "        weightsHiddenToOutput -= learningRate * np.dot(gradient_HiddenToOutput, np.transpose(hiddenLayer_inputs))\n",
    "        weightsInputToHidden -= learningRate * np.dot(gradient_InputToHidden, np.transpose(inputs))\n",
    "\n",
    "        # Store residual\n",
    "        squared_residuals[obs] = np.sum((target-outputLayer_outputs)**2)\n",
    "\n",
    "    inputs = np.array(X).reshape((inputLayer_size, n))  # all observations\n",
    "    target = y.reshape((outputLayer_size, n))\n",
    "    hiddenLayer_inputs = np.dot(weightsInputToHidden, inputs) + biasInputToHidden  # first hidden layer inputs\n",
    "    hiddenLayer_outputs = ReLU(hiddenLayer_inputs)  # hidden layer output (after activation)\n",
    "    outputLayer_inputs = np.dot(weightsHiddenToOutput, hiddenLayer_outputs) + biasHiddenToOutput  # input to output layer\n",
    "    outputLayer_outputs = ReLU(outputLayer_inputs)\n",
    "    iteration_loss_square = mse(target, outputLayer_outputs)\n",
    "    iteration_loss = math.sqrt(iteration_loss_square)\n",
    "    loss_log.append(iteration_loss)\n",
    "\n",
    "    # Development of the loss as average over obs-level losses\n",
    "    print(f'Epoch {iteration} with average error of {iteration_loss:.2f}.')\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that! As the epochs continue, we can see iterations show improvement in errors in the current random batch of observations. Important questions:\n",
    "- Why could it be the case that the error is not continuously falling?\n",
    "- What are some other good ideas for stopping rules for our algorithm? Or how can we manipulate the learning rate?\n",
    "- Instead of looking at average loss per batch, could you think of a better way to measure model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of back propagation\n",
    "So, after setting up the neural network, we need to:\n",
    "- Calculate loss from the forward pass\n",
    "- Calculate the gradient of random observations (SGD)\n",
    "- Update weights by moving down the gradient with a step size specified by your learning rate\n",
    "- Make new predictions with the new weights\n",
    "- Repeat the these steps until you reach a stopping rule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
