{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAMS Tutorial #4: Neural Networking (NN) Primer\n",
    "## Part 2 of 3\n",
    "This is the second of three notebooks which will cover the foundations of neural networks. Recall that every connection in a neural network carries a weight. Therefore, calculating the *forward pass* requires concrete values for these weights. In the first notebook, we discussed how the weights can be initialized randomly. The focus of this notebook is network training. Training is the process in which we update the weights to maximize the fit of our network to (the training) data. Network training is, therefore, equivalent to maximum likelihood estimation in regression analysis. \n",
    "\n",
    "## This notebook's topic: back propagation in neural networks ##\n",
    " 1. Overview of the forward pass\n",
    " 2. Caculating loss\n",
    " 2. Finding a minimum\n",
    " 3. Moving towards a minimum\n",
    " 4. One solver for gradient descent: Stochastic gradient descent\n",
    " 5. Stopping Rules\n",
    " 6. Overview of back propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(888)  # set seed for reproducibility (numPy functions)\n",
    "random.seed(888)  # set seed for reproducibility (random package functions)\n",
    "\n",
    "range_for_demo = np.linspace(-5, 5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate regression dataset\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "n = 1000  # number of observations in our simulation\n",
    "k = 15    # number of features in X in our simulation\n",
    "\n",
    "XX, Y = make_regression(n_samples=n, n_features=k, noise=5, random_state=888)\n",
    "\n",
    "Y = Y.reshape(n, 1)  # Make sure that y is an array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For start, we revisit the forward pass through a neural network. Here, we simply go through the corresponding codes. The first part of this series of notebooks explains the steps in detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture:\n",
    "inputLayer_size = k      # number of features in X\n",
    "hiddenLayer_size = 10    # number of hidden layer nodes\n",
    "outputLayer_size = 1     # number of values to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight initialization for first forward pass\n",
    "\n",
    "limit = np.sqrt(6 / (inputLayer_size + outputLayer_size))  # Recommended weight initialization\n",
    "\n",
    "W_0 = np.random.uniform(-limit, limit, (hiddenLayer_size, inputLayer_size))    # Random weight initialization\n",
    "W_1 = np.random.uniform(-limit, limit, (outputLayer_size, hiddenLayer_size))  # Random weight initialization\n",
    "\n",
    "B_0  = np.ones((hiddenLayer_size,1))  # Bias initialization\n",
    "B_1 = np.ones((outputLayer_size,1))  # Bias initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function that we will use in this scenario and its derivative\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)  # 0 if input is negative, x if input is positive\n",
    "\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (ReLU(x) > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First forward pass\n",
    "X = np.array(XX[5]).reshape((inputLayer_size, 1))  # observation 5\n",
    "y = Y[5].reshape((outputLayer_size, 1))  # true value of target\n",
    "Z_hidden = np.dot(W_0, X) + B_0  # first hidden layer inputs\n",
    "H = ReLU(Z_hidden)  # hidden layer output (after activation)\n",
    "Z_output = np.dot(W_1, H) + B_1  # input to output layer\n",
    "y_hat = ReLU(Z_output)  # activation applied to output layer = prediction!\n",
    "y_hat.shape  # shape of first forward pass predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the loss and target for this observation. We can see that the random weight initialization is pretty far off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]] [[207.75432522]]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "\n",
    "## Motivation:\n",
    "With a set of random weights, would a prediction be any good? Probably not. However, the magic of neural networks comes from the method to update the complex system of weights. This is known as back propagation or, in brief, backprop. \n",
    "\n",
    "## Calculating loss\n",
    "The first step in back propagation is to calculate the loss. Recall that *loss* is just another term for error. Several measures facilitate calculating the loss of a regression models. Well-known candidates include mean square error, root-mean square error, mean absolute error, and many others. In general, a loss function measures the degree to which the outputs of your model (i.e., neural network) fit the training data. Higher losses indicate a poorer fit. Thus, after computing the loss, we want to minimize it. This minimization is what network training is about. \n",
    "<br>\n",
    "For illustration, we consider the mean square error (MSE) in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared error\n",
    "def mse(true, pred):\n",
    "    sse = 0\n",
    "    for i in range(len(true)):\n",
    "        sse += (true[i] - pred[i])**2\n",
    "    mse = (1 / 2*len(true)) * np.sum(sse)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's calculate the loss from our first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21580.929823386057"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(y, y_hat)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a minimum\n",
    "Like a linear regression, we are working with input values along with layers of weights and a biases (= intercepts in the regression). Our loss function is a function of these three elements. Of these three elements, weights and bias are what we can adjust. So, let’s derive our loss function based on those. We will focus on deriving with respect to weights here, but the process for biases is similar.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&\\frac{\\partial MSE }{\\partial w_j} = \\frac{\\partial}{\\partial w_j} \\frac{1}{2n} \\sum_{i=1}^n (\\hat y - y)^2\\\\\n",
    "&\\frac{\\partial MSE }{\\partial w_j} = \\frac{1}{2n} \\sum_{i=1}^n \\frac{\\partial}{\\partial w_j}  (\\hat y - y)^2\\\\\n",
    "&\\frac{\\partial MSE }{\\partial w_j} = \\frac{1}{2n} \\sum_{i=1}^n 2(\\hat y - y) \\cdot \\frac{\\partial}{\\partial w_j}(\\hat y - y)\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Now at this point, we know that the true $y$ is just a scalar. So its derivative is 0. What is the derivative of $\\hat y$ w.r.t. $w_j$? Well, recall the function of $\\hat y$ :\n",
    "\n",
    "$$\\hat y=w_{0}+w_{1}x_{1}+w_{2}x_{2}+...+w_{k}x_{k}$$ \n",
    "\n",
    "So, the derivative w.r.t. to a single $w_j$ will just be the corresponding $x$ value.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "& \\frac{\\partial MSE }{\\partial w_j} = \\frac{2}{2n} \\sum_{i=1}^n (\\hat y - y) \\cdot \\frac{\\partial \\hat y}{\\partial w_j}\\\\\n",
    "&\\frac{\\partial MSE }{\\partial w_j} = \\frac{1}{n} \\sum_{i=1}^n (\\hat y - y) \\cdot x \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "note that if $ w_0 $ is the intercept, we would set $ x=1 $.\n",
    "\n",
    "Consider: Our loss function is a function of the true value and our NN’s output. Our NN’s output is a function of the last nodes’ outputs. Our last nodes’ outputs are all a function of their own input values, weights and biases. We’re lucky that we can simply use the chain rule to calculate the derivatives for each parameter! We will then have to organize our derivatives into gradients which are vectors of derivatives of different variables from the same function. For full optimization, we will need to calculate this derivative for every observation as well. We will discuss a simplification technique for this called stochastic gradient descent later.\n",
    "\n",
    "So for each observation, we have to calculate: \n",
    "\n",
    "$$ \\Bigg[ \\frac{\\partial MSE }{\\partial w_0},  \\frac{\\partial MSE }{\\partial w_1}, \\frac{\\partial MSE }{\\partial w_2}, ... ,  \\frac{\\partial MSE }{\\partial w_k} \\Bigg] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_derivative(true, pred, x):\n",
    "    n = len(x)\n",
    "    loss_d = (1/n) * np.sum((pred-true)*x)\n",
    "    return loss_d.reshape(1, 1)\n",
    "\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    return (x > 0)*1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-34.98183822]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_derivative = mse_derivative(Y[5], y_hat, XX[5])  # loss derivative\n",
    "loss_derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_derivative = ReLU_derivative(Z_output)  # activation derivative\n",
    "activation_derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the output layer's output is a function of the following:\n",
    "- the output layer's inputs\n",
    "- the hidden layer's outputs\n",
    "- the hidden layer's inputs\n",
    "- the original X values\n",
    "\n",
    "We will need to update all weights connecting these together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first weights (1, 10)\n",
      "Shape of gradient vector (10, 1)\n",
      "Shape of input (15, 1)\n"
     ]
    }
   ],
   "source": [
    "# Update weights furthest back in the network (between hidden and output layer)\n",
    "gradient_HiddenToOutput = np.dot(loss_derivative*activation_derivative, np.transpose(H))\n",
    "gradient_HiddenToOutput.shape\n",
    "\n",
    "# Update output layer biases\n",
    "gradient_HiddenToOutput_bias = loss_derivative*activation_derivative\n",
    "\n",
    "# Save the error of the output layer\n",
    "pred_errors = loss_derivative * activation_derivative\n",
    "\n",
    "# Find gradient for next step for backpropagation: gradient to update weights between hidden and input layer\n",
    "gradient_InputToHidden = np.dot(W_1.T, pred_errors)\n",
    "print(f'Shape of first weights {W_1.shape}')\n",
    "\n",
    "# Next propagation backwards: derivative of the hidden layer output wrt the hidden layer input (ReLU derivative)\n",
    "gradient_InputToHidden = gradient_InputToHidden * ReLU_derivative(Z_hidden)\n",
    "print(f'Shape of gradient vector {gradient_InputToHidden.shape}')\n",
    "\n",
    "# Last propagation: derivate of the hidden layer input wrt to the weight matrix connecting the hidden layer to inputs X\n",
    "gradient_InputToHidden = np.dot(gradient_InputToHidden, np.transpose(X))\n",
    "print(f'Shape of input {X.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving towards a minimum\n",
    "We could update all weights by trying to take a large step down the gradient. However, since there are so many combinations of weights and bias, this just reach one local minimum of many, or it may overshoot the local minimum. It could be that there is a lower local minimum that we cannot yet identify. If we take smaller steps down the gradient, update weights/biases, make a prediction again with these new values and recalculate the gradient, we may find a steeper path which indicates the existence of a lower minimum. This step size is called a learning rate. This slow and iterative way of updating weights is called gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a few graphics. The one on the left shows the example of trying to fit a curve to points. The figure on the right is a depiction of slowly moving down a error surface to find a minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
       "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
    "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can come back to our scenario and begin the process of updating our weights based on our $learningRate \\cdot gradient$. The formal equation for updating weights and biases looks like this:\n",
    "$$ w_j' = w_j + lr \\cdot \\frac{\\partial MSE }{\\partial w_j} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_InputToHidden_bias = np.dot(W_1.T, pred_errors) * ReLU_derivative(Z_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent step\n",
    "learningRate = 0.00001  # define some learning rate\n",
    "\n",
    "# Update weights between hidden and output layer (furthest back)\n",
    "W_1 -= learningRate * gradient_HiddenToOutput\n",
    "# Update bias in output layer\n",
    "B_1 -= learningRate * gradient_HiddenToOutput_bias\n",
    "# Update weights between input and hidden layer (furthest forward)\n",
    "W_0 -= learningRate * gradient_InputToHidden\n",
    "# Update bias in hidden layer\n",
    "B_0 -= learningRate * gradient_InputToHidden_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One solver for gradient descent: SGD\n",
    "As you can imagine, the number of computations for this process can be quite large. It requires one derivative per observation per parameter per step! Stochastic gradient descent (SGD) is one way to greatly simplify this task. This method picks samples randomly and only calculates their derivatives for optimization. By the strictest definition, SGD picks one sample per optimization step, but **it is common to optimize with batches of observations**. This could lead to a more stable path to a local minimum. In the code below, we will select a new variable batch_size, which will randomly select samples to optimize parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopping rules\n",
    "What if we find the program is going through too many iterations with little improvement? We need to set a stopping rule to decide when the program should end. There are two very commonly chosen options: specifying a maximum number of epochs (= iterations) allowed or specifying a maximum allowed number of epochs without improvement in the accuracy of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learningRate = 0.00001\n",
    "epochs = 25  # stopping rule, how many corrective iterations we will allow\n",
    "batch_size = 100\n",
    "\n",
    "input_dim = XX.shape[1]  # number of variables\n",
    "output_dim = 1  # should be equal to 1 since we are only finding the predicted value for 1 regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 with average error of 3876.03.\n",
      "Epoch 2 with average error of 3875.81.\n",
      "Epoch 3 with average error of 3875.47.\n",
      "Epoch 4 with average error of 3874.87.\n",
      "Epoch 5 with average error of 3874.23.\n",
      "Epoch 6 with average error of 3873.37.\n",
      "Epoch 7 with average error of 3872.75.\n",
      "Epoch 8 with average error of 3872.12.\n",
      "Epoch 9 with average error of 3871.44.\n",
      "Epoch 10 with average error of 3870.58.\n",
      "Epoch 11 with average error of 3869.90.\n",
      "Epoch 12 with average error of 3868.89.\n",
      "Epoch 13 with average error of 3868.13.\n",
      "Epoch 14 with average error of 3867.41.\n",
      "Epoch 15 with average error of 3866.74.\n",
      "Epoch 16 with average error of 3865.86.\n",
      "Epoch 17 with average error of 3865.31.\n",
      "Epoch 18 with average error of 3865.10.\n",
      "Epoch 19 with average error of 3864.72.\n",
      "Epoch 20 with average error of 3864.30.\n",
      "Epoch 21 with average error of 3863.79.\n",
      "Epoch 22 with average error of 3863.66.\n",
      "Epoch 23 with average error of 3863.45.\n",
      "Epoch 24 with average error of 3863.32.\n",
      "Epoch 25 with average error of 3863.37.\n"
     ]
    }
   ],
   "source": [
    "iteration = 0\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "while iteration < epochs:\n",
    "\n",
    "    # Process one batch of random observations per epoch/iteration\n",
    "    random_batch = random.sample(range(0, XX.shape[0]), batch_size)  # choose observations for random batch\n",
    "\n",
    "    # Update weights and biases one at a time with each random observation's error (all steps before, just in a loop together)\n",
    "    for obs in random_batch:\n",
    "\n",
    "        # Select feature values and target value for random observation\n",
    "        X = np.array(XX[obs]).reshape((input_dim, 1))\n",
    "        y = Y[obs].reshape((1, 1))\n",
    "\n",
    "        # Compute the forward pass through the network all the way up to the final output\n",
    "        Z_hidden = np.dot(W_0, X) + B_0\n",
    "        H = ReLU(Z_hidden)\n",
    "        Z_output = np.dot(W_1, H) + B_1\n",
    "        y_hat = ReLU(Z_output)  # prediction from first iteration\n",
    "\n",
    "        # Gradient for the weights between hidden and output layers\n",
    "        gradient_HiddenToOutput = mse_derivative(Y[obs], y_hat, XX[obs]) * ReLU_derivative(y_hat)\n",
    "        pred_errors = gradient_HiddenToOutput\n",
    "\n",
    "        # Gradient for the weights between input and hidden layers\n",
    "        gradient_InputToHidden = np.dot(W_1.T, pred_errors) * ReLU_derivative(Z_hidden)\n",
    "\n",
    "        # Update biases according to learning rate and gradient\n",
    "        B_1 -= learningRate * gradient_HiddenToOutput\n",
    "        B_0 -= learningRate * gradient_InputToHidden\n",
    "\n",
    "        # Update weights according to learning rate and gradient\n",
    "        W_1 -= learningRate * np.dot(gradient_HiddenToOutput, np.transpose(Z_hidden))\n",
    "        W_0 -= learningRate * np.dot(gradient_InputToHidden, np.transpose(X))\n",
    "\n",
    "    # Check how well the model does on all observations by passing them through a forward pass with most up to date weights\n",
    "    XX_reshaped = np.array(XX).reshape((inputLayer_size, n))  # all observations\n",
    "    Y_reshaped = Y.reshape((outputLayer_size, n))\n",
    "    Z_hidden = np.dot(W_0, XX_reshaped) + B_0  # first hidden layer inputs\n",
    "    H = ReLU(Z_hidden)  # hidden layer output (after activation)\n",
    "    Z_output = np.dot(W_1, H) + B_1  # input to output layer\n",
    "    Y_hat = ReLU(Z_output)\n",
    "    \n",
    "    # Calculate loss for entire model\n",
    "    iteration_loss_square = mse(Y_reshaped, Y_hat)\n",
    "    iteration_loss = math.sqrt(iteration_loss_square)\n",
    "    loss_log.append(iteration_loss)\n",
    "\n",
    "    # Development of the loss as average over observation-level losses\n",
    "    print(f'Epoch {iteration+1} with average error of {iteration_loss:.2f}.')\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot losses\n",
    "plt.plot(loss_log)\n",
    "plt.title(\"Loss over Iterations of NN\")\n",
    "plt.xlabel(\"Iteration Number\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at that! As the epochs continue, we can see iterations show improvement in errors in the current random batch of observations. Important questions:\n",
    "- Why could it be the case that the error is not continuously falling?\n",
    "- What are some other good ideas for stopping rules for our algorithm? Or how can we manipulate the learning rate?\n",
    "- Instead of looking at average loss per batch, could you think of a better way to measure model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of back propagation\n",
    "So, after setting up the neural network, we need to:\n",
    "- Calculate loss from the forward pass\n",
    "- Calculate the gradient of random observations (SGD)\n",
    "- Update weights by moving down the gradient with a step size specified by your learning rate\n",
    "- Make new predictions with the new weights\n",
    "- Repeat the these steps until you reach a stopping rule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
