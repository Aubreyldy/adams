{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT_Tutorial_(and_a_bit_of_flair).ipynb","provenance":[],"collapsed_sections":["CVTDahv-JdeH","f8kNUN2jwxPK"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m7hlzW2mvoJd"},"source":["# BERT Tutorial\n","\n","This tutorial is meant to teach the usage of BERT. The task at hand will be sentiment analysis of movie reviews. We will be following the [official tutorial](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) from tensorflow which is one of the main ways we can use BERT to solve problems, along with pytorch. BERT and transfomer technology in general has been a revolution in natural language processing (NLP) so it is very important to understand how it works to take advantage of its capabilities."]},{"cell_type":"markdown","metadata":{"id":"89N_k__4b-Vg"},"source":["## Review of other methods in NLP\n","\n","There has been astonishing progress made in the field of natural language processing. If you used a free translation software 10 years ago and compare that to something like [DeepL](https://www.deepl.com/en/translator) or [Google translate](https://translate.google.com/)'s performance today, this should tip you off that the field has seen some large changes. We know that machines work in numbers, so our natural language has to somehow be coded this way for  computers to process. How do we do this with language? There have been several popular ways:"]},{"cell_type":"markdown","metadata":{"id":"zfh-_Wflt5oq"},"source":["### Bag of Words\n","Probably the most popular still today is the Bag of Words model (BoW). While it is a bit crude, it is simple which is extremely attractive while other models take a long time to understand and set up. The basic logic of this model is that texts are based around keywords which are most indicative of their meaning. A vector is created which has the same length as the total number of unique tokens in the text. For example, if all texts in the sample include 50 unique words and 5 unique punctuation characters, the vector will have a length of 55 with each position in the vector indicating whether a single word or piece of punctuation is present in that observation of the text.  \n","\n","As you could probably imagine, this means that large datasets would have very large vectors which can be problematic. To prevent this, stop words are removed (and, the, a, but) and word inflections are dropped by either stemming or lemmatization (acts, acting, acted all become act). We can also drop words that appear too many or too few observations as they would probably not contribute much to the model's predictive power.\n","\n","Even after doing this, the problem of large dimensionality still often remains a curse for this model. Another very large drawback is the loss of context for words in terms of their position in the sentence. We can partially solve this by not only considering the presence of a word, but also word pairs or in sets of three. This is called n-grams (with n being the number of neighbors to consider). For example, in a 2-gram model, the observation \"I like cats.\" would generate a position in the vector for \"I\", \"like\", \"cats\", \"I like\", \"like cats\". We can then drop instances that are too common or too rare as before.\n","\n","Even n-grams cause a loss of information which could cause critical problems during analysis. As such, BoW does have pretty large flaws due to its simplicity. Nevertheless, it remains a very popular model."]},{"cell_type":"markdown","metadata":{"id":"yUIuZK8Xt7ey"},"source":["### Long Short-Term Memory Networks\n","Another popular model is the version of a recurrent neural network (RNN) called long short-term memory (LSTM). Instead of using a vector which indicates the presence of a token, texts are preprocessed into sequences so word order now has an importance. These networks are able to optimize based on looking at these sequences (they're also great for time series data in this way), and considering the relations of words to one another based on what appears next in sequence. The drawback with this is that they normally only consider tokens which proceed. Bidirectional LSTMs solve this problem by looking at proceeding tokens then after at preceding tokens in the sequence. However, it still does this in separate steps. A good method to unify these steps to truly learn the \"meaning\" of a sentence has been lacking in NLP."]},{"cell_type":"markdown","metadata":{"id":"92VtI-CFt9xE"},"source":["### Embeddings\n","Do words have some hidden characteristics that can be extracted? Think of what we'd need to somehow extract for a computer to perform these operations correctly:\n","- \"Canada\" + \"capital\" = \"Ottawa\"\n","- \"man\" - \"old\" = \"boy\"\n","- \"computer\" + \"portable\" = \"laptop\"\n","\n","If this is possible, how could we create some kind of latent representation of the meaning of not only words, but sentences and whole documents too? It may be a surprise, but this has been done and it can massively help the performance of NLP processes.\n","\n","One of the best examples of it is the [Global Vectors for Word Representation or GloVe](https://nlp.stanford.edu/projects/glove/). It is trained to learn the co-occurence of words in an extremely large number of texts. Through this process, the Stanford team was able to create a set of embeddings that represent the latent features of words. \n","\n","As these representations are numerical, they can be used in any machine learning algorithm in an attempt to solve the problem at hand. Creating smart ways to combine individual word embeddings or specializing word embeddings to a specific domain could be helpful for the task. We can then use thes embeddings as regular numerical features in machine learning algorithms."]},{"cell_type":"markdown","metadata":{"id":"jTpGnKP8QSy0"},"source":["## What is BERT?\n","\n","Bidirectional Encoder Representations from Transformer or BERT is a language model which, similar to other transformer methods like GPT-3, has shown to be most promising in capturing the meaning of text. Not only is it more accurate than previous methods, it can be used in several different ways to complete multiple NLP tasks.\n","\n","It works on the basis of transformers and attention mechanisms. Put plainly, each piece of text is broken down into tokens to be processed by the model. Tokens include things like words, numbers and punctuation (non-alphanumeric characters). Through training, BERT learns the association that tokens have with one another by looking at their proximity to other tokens in sample text. In the end, after being pre-trained for hours on we consider now to be very powerful processors, new text can be fed into the model and it can ascertain meaning from it. \n","\n","BERT is trained two ways: next sentence prediction (NSP) and masked language modeling (MLM). In the first task, BERT shuffles sentences and learns which ones follow the other. In the second task, BERT hides a random token and gets trained on what it should be. The fascinating part of this model is its ability to learn the context of words in both directions, something that was hard for previous NLP models to do. \n","\n","The text gets represented in BERT in a very abstract form called an embedding vector. This vector for BERT is of a fixed length of 768, so no matter how long the text is, the representation in an embedding will always be the same size. What do each of these values in the vector mean? We can't be 100% sure. They are some way BERT has found to represent the meaning of the text in numeric form. The best we can do is to check the similarity of vectors using a metric (for example, cosine similarity) to try to determine what in the vector changed.\n","\n","There are [many tasks](https://huggingface.co/transformers/task_summary.html) that BERT can solve:\n","- Sequence classification (classify a sequence of text, eg. positive or negative)\n","- Extractive answering (locate the answer to a question in a text)\n","- Language modeling (learn the relations of words given sample text, eg. financial texts)\n","- Text generation\n","- Name entity recognition (NER, locate proper nouns and classify them)\n","- Summarization\n","- Translation \n","\n","Our goal will be to take sequences of text and determine if they are positive or negative, so our task is sequence classification. For this purpose, we could use two methods to try to solve the task:\n","- Directly use BERT for the supervised task\n","- Extract embeddings from BERT and use those as features in another model\n","\n","The first method involves using BERT directly to solve a task. This is also often called transfer learning if we start applying BERT to a task that it was not trained on in the first place. \n","\n","In the second method, we take advantage of the pre-trained BERT models only to transform our text into numeric features for future machine learning tasks. In this way, we can add more features if we would like as well. For example, if we were trying to model stock volatility and we wanted to use news reports and macroeconomic variables like the leading index as inputs, then we could extract embeddings, throw in our other numeric variables then use it all as inputs into a neural network.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gboIcX8X-QHg"},"source":["# Practical example of BERT"]},{"cell_type":"markdown","metadata":{"id":"IN6Nz17SJJVi"},"source":["## Install transformers\n","Unlike many other libraries, Colab does not have the transformers package pre-installed. You will have to install it every time that you start Colab again. This is the package where you will find most of the critical tools for BERT including the pre-trained models and tokenizer."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zR0LXCwYHP8l","outputId":"424190c0-6a17-4b6e-8886-b7aeca337661"},"source":["pip install -q -U tensorflow-text"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 4.3MB 4.3MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"omuo6DMTVNPQ","outputId":"a1d77d13-691e-4ba9-9b66-79e47c536cf3"},"source":["pip install -q tf-models-official"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 1.6MB 4.3MB/s \n","\u001b[K     |████████████████████████████████| 645kB 24.6MB/s \n","\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n","\u001b[K     |████████████████████████████████| 358kB 30.1MB/s \n","\u001b[K     |████████████████████████████████| 61kB 7.2MB/s \n","\u001b[K     |████████████████████████████████| 38.2MB 83kB/s \n","\u001b[K     |████████████████████████████████| 102kB 11.7MB/s \n","\u001b[K     |████████████████████████████████| 686kB 28.3MB/s \n","\u001b[K     |████████████████████████████████| 215kB 40.5MB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CVTDahv-JdeH"},"source":["## Import libraries\n","Now that you have the transfomers library on hand, it will be necessary to import it and the rest of the libraries that you will need in the task. Here we will need tensorflow, pandas, OS and shutil for basic tasks and also specific parts of the transformers package for BERT."]},{"cell_type":"code","metadata":{"id":"XVObl75mHYU1","executionInfo":{"status":"error","timestamp":1624615450842,"user_tz":-120,"elapsed":1450,"user":{"displayName":"Stefan Lessmann","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GihtuVUIO07jrZ6NKEVggi44vrPvluMzUCsHoZh=s64","userId":"06342662613942148717"}},"outputId":"f3f732ed-b9ef-428c-9547-c9161cba6317","colab":{"base_uri":"https://localhost:8080/","height":399}},"source":["import os\n","import shutil\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub # for BERT models\n","import tensorflow_text as text\n","from official.nlp import optimization  # for AdamW optimizer\n","\n","import matplotlib.pyplot as plt\n","\n","tf.get_logger().setLevel('ERROR')"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-5d9f385ce9c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m \u001b[0;31m# for BERT models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mofficial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization\u001b[0m  \u001b[0;31m# for AdamW optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_text'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"g0Zn8NeZJ0DK"},"source":["## Load and set up the dataset\n","\n","In this task, we will be using the IMDB reviews dataset. This dataset includes 50,000 IMDB reviews of movies. 25k are allocated for training and another 25k for testing. These reviews are labelled positive or negative. It will be our model's task to read a new review and determine which sentiment it projects. There are an additional 50,000 unlabelled reviews, but since this is a supervised learning task, we will be discarding these."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7PdO69tUHUJd","outputId":"7b2b9ebb-aec5-4f5e-880a-0e2e96b3220c"},"source":["url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n","\n","df = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n","                                  untar=True, cache_dir='.',\n","                                  cache_subdir='')\n","\n","df_dir = os.path.join(os.path.dirname(df), 'aclImdb')\n","X_train_dir = os.path.join(df_dir, 'train')\n","X_test_dir = os.path.join(df_dir, 'test')\n","\n","# we only need labeled data (data for supervised learning), so we can remove the unsupervised folder\n","remove_dir = os.path.join(X_train_dir, 'unsup')\n","shutil.rmtree(remove_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","84131840/84125825 [==============================] - 5s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e_uktomnWxT5"},"source":["BERT is going to take up a lot of processing power. It is highly advisable to organize your data into batches so that the amount of data that you are working with is manageable. For now, we will set the size of the batches of data that we will take to 32. You can experiment with this number when working with the program for later tasks."]},{"cell_type":"code","metadata":{"id":"fSaj23L2XDll"},"source":["batch_size = 32"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ioiqFl_pKLAZ"},"source":["We will first organize the dataset a little to make it more useful for our task. As with all machine learning tasks, this part is likely going to be the most tedious in you to set up for your own projects. However, ensuring that your data is clean and makes sense is absolutely critical for the next step which is feeding it into the model.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5wzMxyhMXJBq"},"source":["This data only comes with a train and a test set. In deep learning, it is very useful to have a validation set as well. The machine will be trained using data on the training set, performance will be checked on the validation set. Finally, the test set, which is completely new to the model, will give us a final evaluation of the model's performance. Let's first create this validation set so that we can check our model the conventional way with an 80-20 split on the training set.\n","\n","Note that the function [prefetch](https://www.tensorflow.org/guide/data_performance#prefetching) is just used to prepare the data as the machine would expect to receive it. It is normally used to make sure that the next batch of data is ready for use."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lcz48LGgHdAv","outputId":"ac210662-8f8c-47bd-cee6-919035a69edf"},"source":["# set seed for reproducibility in train-test split\n","seed = 888\n","\n","# Create the pre-processing train df and create a seperate subset training only\n","X_train_raw = tf.keras.preprocessing.text_dataset_from_directory(\n","    X_train_dir,\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset='training',\n","    seed=seed)\n","\n","X_train = X_train_raw.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","# Note: AUTOTUNE tracks the time spent in each operation, these times can be fed into the optimization algorithm\n","\n","# Take the validation data subset for processing\n","X_val = tf.keras.preprocessing.text_dataset_from_directory(\n","    X_train_dir,\n","    batch_size=batch_size,\n","    validation_split=0.2,\n","    subset='validation',\n","    seed=seed)\n","\n","X_val = X_val.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n","\n","# Prepare the test data for processing\n","X_test = tf.keras.preprocessing.text_dataset_from_directory(\n","    X_test_dir,\n","    batch_size=batch_size)\n","\n","X_test = X_test.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 25000 files belonging to 2 classes.\n","Using 20000 files for training.\n","Found 25000 files belonging to 2 classes.\n","Using 5000 files for validation.\n","Found 25000 files belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zIytyzBwMf0j"},"source":["Let's take a quick look at three reviews to understand our data a bit better. We see that they are labeled and quite detailed. Do you agree with their labels? We see that label 1 corresponds with positive and 0 with negative. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dgoWJpUrHptR","outputId":"8d091ace-f322-4b85-8969-b412fe609836"},"source":["# Get classes\n","class_names = X_train_raw.class_names\n","\n","for text_batch, label_batch in X_train.take(1):\n","  for i in range(3):\n","    print(f'Review: {text_batch.numpy()[i]}')\n","    label = label_batch.numpy()[i]\n","    print(f'Label : {label} ({class_names[label]})')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Review: b'I remember stumbling upon this special while channel-surfing in 1965. I had never heard of Barbra before. When the show was over, I thought \"This is probably the best thing on TV I will ever see in my life.\" 42 years later, that has held true. There is still nothing so amazing, so honestly astonishing as the talent that was displayed here. You can talk about all the super-stars you want to, this is the most superlative of them all!<br /><br />You name it, she can do it. Comedy, pathos, sultry seduction, ballads, Barbra is truly a story-teller. Her ability to pull off anything she attempts is legendary. But this special was made in the beginning, and helped to create the legend that she quickly became. In spite of rising so far in such a short time, she has fulfilled the promise, revealing more of her talents as she went along. But they are all here from the very beginning. You will not be disappointed in viewing this.'\n","Label : 1 (pos)\n","Review: b'I\\'ve seen a lot of movies and rarely would I ever rate a movie \"1\" but this movie was beyond terrible.<br /><br />The acting was terrible, the plot was ridiculous, the effects were unrealistic and the characters were annoying. Usually when I watch scary movies I think it\\'s DUMB when the characters hears a noise in house/forest/school/etc and then yells out \"hello is anyone there?\" - but at least they\\'re believable when they do it.. This movie couldn\\'t even get that right.<br /><br />This is a movie that\\'ll make other B-horror movies like Venom and The Fog look like academy award winning masterpieces.<br /><br />I always have an open mind while watching movies and I can only say that this movie was a complete waste of time and I write this comment so that anyone else who\\'s thinking of watching this movie will think again. IT\\'S AWFUL!'\n","Label : 0 (neg)\n","Review: b\"This movie stinks! You will want back the two-plus hours it takes to get through it. Sliding Doors, w/ Gwyenth Paltrow and directed by Peter Howit, did what Melinda & Melinda tries to do much much MUCH better. That movie was clever, witty, and well-acted. I cared about what happened to both Gwyenths -- or rather the characters she played -- and the performances by supporting cast were fantastic.<br /><br />Where as Melinda & Melinda is tiresome, the dialogue is contrived and I could have cared less about any of these people -- least of all Melinda. One Melinda is so dysfunctional -- her first glass of wine is at 10 a.m. -- and so melodramatic she is laughable, and not in the comedic sense. The 2nd Melinda is fine, but forgettable.<br /><br />Woody Allen's previous ensemble movies worked because, I'm guessing, he spent time on the screenplay and the actors were talented. One piece of trivia for this movie is that he wrote this screenplay in two months: you can tell. And while Chloe Sevigny is talented -- those around her are not, not enough to be a whole presence. The movie ends up being Chloe Sevigny and a bunch of other people you know you've seen in other movies but can't quite remember which ones.<br /><br />Sad, very sad.\"\n","Label : 0 (neg)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lOHw_TJ7WWvj"},"source":["## Preprocessing text for BERT\n","\n","Now we get to a critical step with using BERT: determining which version is needed. According to the type of BERT that we will be using, we will need to pre-process our text accordingly. We will now explore what BERT does in the pre-procesing state so that you can see the inputs of the model.\n","\n","You can explore a [large list of versions of BERT here](https://huggingface.co/transformers/pretrained_models.html). These pretrained versions of BERT differ mainly in size and/or the topics of text. As you can imagine, different words have very different meanings if we change the context. For example, while a liability may be a negative word in a general context, it is relatively neutral in the world of finance. As such, using specific versions of BERT can sometimes help with the performance of your model, though this is not always the case. It is a very good idea to test several versions of BERT for your purposes to see which one is optimal for your situation.\n","\n","For our purposes, we will used a **small uncased BERT**. Here, uncased means that BERT will ignore capitalization and small means that BERT will only take shorter inputs. There are many more versions of BERT to try, so take a quick look through the options on the huggingface library to see if there are any more that suit your purposes."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cjlmAO2JZOut","outputId":"59a026f2-e438-4c79-af9a-15436d72dec5"},"source":["bert_preprocessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:absl:Using /tmp/tfhub_modules to cache modules.\n","INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'.\n","INFO:absl:Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3, Total size: 1.96MB\n","INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"N3f3W5A9Xl3p"},"source":["Now let's take a look at how pre-processing stages transform a text input. We start with a simple fabricated review. We see that BERT preprocessing will generate several values for this text: an input type ID, an input word ID and an input mask.\n","\n","BERT is trained two ways: next sentence prediction (NSP) and masked language modeling (MLM). In the first task, sentences get shuffled and BERT must determine if it is shown sentences in the correct order or not. In the second task, BERT hides a random token and gets trained on what it should be. Through these two steps, BERT gets an idea of the meaning of words in a sentence-level and broader context.\n","\n","When we look at `input_word_id`'s shape, we see that it is a vector which has a length of 128. BERT in general will truncate all texts to 512 tokens, however small BERT has a limit of 128 tokens. So, it is important to pay attention to the lengths of texts which the model will receive as your model may ignore a large part of them.\n","\n","BERT has already encountered most words that will be important for classification through its pre-training. Each of these words were given an ID so that their meaning (embedding) can be easily looked up. We see the first 12 IDs of the words that are in our text corresponding to each token we input. Was the number of non-zero tokens what you expected? You may have only anticipated the following tokens: `'hated', 'every', 'minute', 'of', 'it', '.'`. Why do we have an extra 2 tokens? BERT automatically adds tokens to indicate the beginning and end of a sentence as well. The rest of the sequence will be 0s as padding to keep the input length the same which is necessary for mathematical convenience.\n","\n","So you understand the other parts of a BERT preprocessor output, `input_mask` elements contain 1 if the word ID is not a padding value and 0 for padding. Lastly, `input_type_ids` can distinguish if the input contains multiple segments."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZtYbKVloWNOd","outputId":"446f1de4-ed05-444c-bb17-c7c5aac26c6a"},"source":["example_text = ['hated every minute of it.']\n","example_preprocessed = bert_preprocessor(example_text)\n","\n","print(f'Shape of input_word_ids: {example_preprocessed[\"input_word_ids\"].shape}')\n","print(f'First 12 input_word_ids: {example_preprocessed[\"input_word_ids\"][0, :12]}')\n","print(\"\")\n","print(f'First 12 elements of input_mask: {example_preprocessed[\"input_mask\"][0, :12]}')\n","print(f'First 12 elements of input_type_ids: {example_preprocessed[\"input_type_ids\"][0, :12]}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape of input_word_ids: (1, 128)\n","First 12 input_word_ids: [ 101 6283 2296 3371 1997 2009 1012  102    0    0    0    0]\n","\n","First 12 elements of input_mask: [1 1 1 1 1 1 1 1 0 0 0 0]\n","First 12 elements of input_type_ids: [0 0 0 0 0 0 0 0 0 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iQrm27jOfLad"},"source":["# Load a BERT model\n","\n","Now that we have pre-processed our text, we are ready to put it into a BERT model! We must first choose the corresponding model which fits with our pre-processed text or we may encounter an error. For example, if we preprocessed our text with regular BERT pre-processing, the length of sequences would be much higher than our current example with small BERT is designed to handle."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87osaWAScP8N","outputId":"d6ab422a-510a-48c1-f5da-6e5bef3cd9cb"},"source":["bert_model = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1')\n","example_bert_results = bert_model(example_preprocessed)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'.\n","INFO:absl:Downloaded https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1, Total size: 115.55MB\n","INFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"RmnjshemgzoG"},"source":["Just as we looked at the outputs for pre-processing, let's now also look at what the BERT model outputs. There are three major parts of this output to explore: `sequence_output`, `pooled_output` and `encoder_outputs`.\n","\n","The first is the `sequence_output`. It represents each individual token in the context in which it appears. BERT will try to determine which homonym is being used and the correct embedding which corresponds with it. For example, the \"chair\" of the committee and the \"chair\" in the room should theoretically be represented by different embeddings and BERT will recognize which to use by the context. The shape of this figure is [no. samples, sequence length, embedding size]. Using this representation of the text, you can then choose how to pool meanings from the text together on your own. However, the next output simplifies this task."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DRZVVwoY1WUC","outputId":"4d4d0ed9-3ff6-4bd6-bf0e-2f18839302b2"},"source":["print(f'Shape of sequence_output:{example_bert_results[\"sequence_output\"].shape}')\n","print(f'First 12 sequence_output values:{example_bert_results[\"sequence_output\"][0, :12]}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape of sequence_output:(1, 128, 512)\n","First 12 sequence_output values:[[-4.60336387e-01  4.33847487e-01  1.18683994e-01 ... -9.53514993e-01\n","   7.37923205e-01 -6.43590450e-01]\n"," [-5.87142050e-01  3.16169351e-01  9.60818768e-01 ... -5.75265288e-01\n","  -8.11677992e-01 -1.82853326e-01]\n"," [-1.07364058e+00  6.57025933e-01  1.46334797e-01 ... -7.22161353e-01\n","  -4.58198398e-01 -6.60157740e-01]\n"," ...\n"," [-1.71589270e-01 -1.15068585e-01  1.20160602e-01 ...  3.12618911e-04\n","   2.55718857e-01 -2.88308144e-01]\n"," [-1.27909139e-01  2.15944901e-01 -1.25321209e-01 ... -1.68895349e-02\n","   4.44979668e-01 -1.68881983e-01]\n"," [ 9.95172337e-02  4.62705642e-02 -9.97777134e-02 ...  3.77714932e-02\n","   7.97547281e-01 -2.85572946e-01]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CRGNLQva1YYe"},"source":["If you want to use the optimal way BERT engineers decided to pool the output of each individual token in the text input, you can use the default `pooled_output`. With this, the entire text is converted into a single vector-style embedding. It will have a shape of [no. samples, embedding size]. This can be one of the more useful outputs if you would like to use these embeddings then in any machine learning algorithm to perform a classification task. Or, we can continue with BERT."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3LEcLguuc6lI","outputId":"f79b9bcb-a5b9-433e-aa7c-6342f2ac5dda"},"source":["print(f'Shape of pooled_output:{example_bert_results[\"pooled_output\"].shape}')\n","print(f'First 12 pooled_output values:{example_bert_results[\"pooled_output\"][0, :12]}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Shape of pooled_output:(1, 512)\n","First 12 pooled_output values:[ 0.9987048  -0.33625588  0.3009688   0.5842965   0.18947738  0.9930429\n","  0.9994944  -0.9786831  -0.47437972 -0.99929345 -0.19990227 -0.98462075]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oiaaC3bH1b5B"},"source":["The last output of BERT are the `encoder_outputs`. They are all of the interim activations of the transformer blocks in BERT. It has a size of [no. samples, sequence length, 1024] with the last sequence being the `sequence_output`. If you would like to use one of the intermediary steps or inspect how BERT is coming up with a specific sequence output, this could be useful."]},{"cell_type":"markdown","metadata":{"id":"Vlq84BaGzVu_"},"source":["## Build the classifier\n","Now we will stack some layers to create a classifier model. We will use:\n","- an input layer which receives the raw text\n","- a layer to preprocess the text for the BERT encoder\n","- an encoding layer which returns BERT outputs\n","- a dropout layer to prevent overfitting\n","- a final dense layer for the final classification"]},{"cell_type":"code","metadata":{"id":"daXr7yxOdL2T"},"source":["def build_classifier_model():\n","  # create input layer\n","  input_layer = tf.keras.layers.Input(shape=(), dtype=tf.string, name='input text')\n","  # add preprocessing layer and input text\n","  preprocessor = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3', name='preprocessing')\n","  encoder_inputs = preprocessor(input_layer)\n","  # add encoding layer and feed preprocessed text into layer\n","  encoder = hub.KerasLayer('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1', trainable=True, name='BERT_encoder')\n","  outputs = encoder(encoder_inputs)\n","  # take the pooled output and apply a dropout layer to it to prevent overfitting\n","  pooled = outputs['pooled_output']\n","  pooled = tf.keras.layers.Dropout(0.1)(pooled)\n","  # create output layer which is the final classifier\n","  pooled = tf.keras.layers.Dense(1, activation=None, name='classifier')(pooled)\n","  return tf.keras.Model(input_layer, pooled) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxY6ApfR70Ih"},"source":["Let's instantiate a model and test it quickly with the example text that we wrote earlier. Remember that the model isn't trained yet, so the first time that we run this, it may not be too accurate. We can try running it again after the training process and see if accuracy improves. Remember that if the model predicts close to 1, it is predicting that the text is positive."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-qOhxPEySav","outputId":"05f5174d-8e5c-4f70-ecfc-bae49a0daf6b"},"source":["classifier_model = build_classifier_model()\n","\n","bert_raw_result = classifier_model(tf.constant(example_text))\n","print(tf.sigmoid(bert_raw_result))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor([[0.64043146]], shape=(1, 1), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"brIM2QM98DUL"},"source":["Now it's time to set up how we will be optimizing the model and evaluating its performance. The loss function is used for optimization. This is the function that will get minimized by the optimizer. We will need [binary cross entropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/BinaryCrossentropy) because we have two classes. If we had more, we would need to use [categorical cross entropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy).\n","\n","A metric is used to judge the performance of your model. This is only for the analyst to observe and has nothing to do with the optimization process. We will be using [binary accuracy](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy) to judge this model's performance."]},{"cell_type":"code","metadata":{"id":"BWaD2Xr85yas"},"source":["loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","metrics = tf.metrics.BinaryAccuracy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0XyNQrxG9Pvb"},"source":["# Setting up meta parameters\n","\n","BERT usually gets trained for less epochs than traditional deep learning models. Depending on your task and system abilities, you can of course experiment with adding more epochs to see how it affects the model's performance. \n","\n","`steps_per_epoch` is the total number of steps (batches of observations) to yield from generator before declaring one epoch finished and starting the next epoch. We will set his equal to the cardinality (the unique items per column) as recommended by tensorflow.\n","\n","We will keep our learning rate at the highest level for the first 10% of training steps then it will follow a linear decay. According to the paper on BERT, you can also try learning rates of 5e-5 and 2e-5 if you'd like to experiment, but these seem to be best for fine-tuning BERT.\n","\n","Lastly, for an optimizer, AdamW will be used, which is Adaptive Movements with weight decay (instead of regular Adam which is based on moments)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lWtLED_I5zwj","outputId":"dfba5a45-4d9a-4fc9-9723-7d96e55eb621"},"source":["epochs = 5\n","steps_per_epoch = tf.data.experimental.cardinality(X_train).numpy()\n","num_train_steps = steps_per_epoch * epochs\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","init_lr = 3e-5 # Best options for BERT: 5e-5, 3e-5, 2e-5\n","optimizer = optimization.create_optimizer(init_lr=init_lr,\n","                                          num_train_steps=num_train_steps,\n","                                          num_warmup_steps=num_warmup_steps,\n","                                          optimizer_type='adamw')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INFO:absl:using Adamw optimizer\n","INFO:absl:gradient_clip_norm=1.000000\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"xpSBUx-Uy6Eg"},"source":["Now that we have all of these set, we can compile the model with them."]},{"cell_type":"code","metadata":{"id":"alnfraF-52sk"},"source":["classifier_model.compile(optimizer=optimizer,\n","                         loss=loss,\n","                         metrics=metrics)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xjysAvFqsFfE"},"source":["Finally we can fit the model! Let's save each epoch's information in a variable called history. By default, our model will display its training progress so we can monitor its performance."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uBOOgBA754hT","outputId":"fc5fafbc-670d-4f29-def1-aa1b81cf8185"},"source":["history = classifier_model.fit(x=X_train,\n","                               validation_data=X_val,\n","                               epochs=epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","625/625 [==============================] - 291s 455ms/step - loss: 0.4778 - binary_accuracy: 0.7506 - val_loss: 0.3918 - val_binary_accuracy: 0.8266\n","Epoch 2/5\n","625/625 [==============================] - 279s 447ms/step - loss: 0.3282 - binary_accuracy: 0.8557 - val_loss: 0.4097 - val_binary_accuracy: 0.8438\n","Epoch 3/5\n","625/625 [==============================] - 279s 446ms/step - loss: 0.2454 - binary_accuracy: 0.8994 - val_loss: 0.4498 - val_binary_accuracy: 0.8352\n","Epoch 4/5\n","625/625 [==============================] - 279s 446ms/step - loss: 0.1900 - binary_accuracy: 0.9240 - val_loss: 0.4683 - val_binary_accuracy: 0.8502\n","Epoch 5/5\n","625/625 [==============================] - 278s 445ms/step - loss: 0.1513 - binary_accuracy: 0.9442 - val_loss: 0.4858 - val_binary_accuracy: 0.8526\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rG8MJIk8sZ4B"},"source":["## Evaluating the model\n","It appears that the validation accuracy was decreasing. This is very promising. Let's see though how the model performa on data that it has never seen before. We see that the model's performance is pretty decent with the test set, accurately classifying more than 85% of instances correctly!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kEfYMohl55pc","outputId":"a1b4e88c-bac0-4292-d1c3-5b46c44aef04"},"source":["loss, acc = classifier_model.evaluate(X_test)\n","\n","print(f'Loss: {loss}')\n","print(f'Accuracy: {acc}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["782/782 [==============================] - 145s 186ms/step - loss: 0.4523 - binary_accuracy: 0.8591\n","Loss: 0.4522794783115387\n","Accuracy: 0.8590800166130066\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F0LpOpynzO24"},"source":["Lastly, let's plot the classic graph of training vs. validation accuracy. We see here a healthy shape as both increase as epochs progress. This demonstrates that training is having a productive effect on the model as it is doing better with reviews that it isn't being trained with."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"wbQUMizvbDsC","outputId":"0b472a58-4f98-491e-8e43-12e9f2ce6661"},"source":["history_dict = history.history\n","\n","acc = history_dict['binary_accuracy']\n","val_acc = history_dict['val_binary_accuracy']\n","\n","epoch_number = range(1, len(acc) + 1)\n","\n","plt.plot(epoch_number, acc, 'r', label='Train accuracy')\n","plt.plot(epoch_number, val_acc, 'b', label='Val accuracy')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend(loc='lower right')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f4a982f1750>"]},"metadata":{"tags":[]},"execution_count":26},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"A6LK6U7YDirs"},"source":["When we look at the loss values instead, we see that validation loss actually increases as epochs continue. Given that accuracy is slightly increasing in the validation set, this may still be acceptable. However, this is not the most ideal situation."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"-zAiQOnBDIeE","outputId":"e9ea8452-4779-4f6d-876e-903ed67a8a42"},"source":["loss = history_dict['loss']\n","val_loss = history_dict['val_loss']\n","\n","plt.plot(epoch_number, loss, 'r', label='Train loss')\n","plt.plot(epoch_number, val_loss, 'b', label='Val loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f4a982aed10>"]},"metadata":{"tags":[]},"execution_count":27},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"yg2XlEs1jXSl"},"source":["# Predict on new text\n","Now that we've tuned the model a bit to our data set, now let's try again with the same sample text that we used earlier (remember 0 is a negative review, 1 is a positive review). You should see some more accuracy compared to our pre-tuned model's performance, especially for less tricky example sentences."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VGTM03Q1jh5a","outputId":"3e225fb3-d3b1-4ea9-9408-ccf1a538ac12"},"source":["bert_raw_result = classifier_model(tf.constant(example_text))\n","print(tf.sigmoid(bert_raw_result))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor([[0.34513733]], shape=(1, 1), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GEO-KwTXzXpf"},"source":["# Concluding\n","\n","Natural language processing is a fascinating field which just went through a pretty big revolution with these transformer models. It will be more and more important to get used to them and learn how to use them on your own. It is likely they will be around for a little bit until the next big revolution in the field. For now, it will be exciting to see how BERT and GPT models continue to evolve and capture more understanding of something as complex as human language."]},{"cell_type":"markdown","metadata":{"id":"f8kNUN2jwxPK"},"source":["# Flair\n","\n","Developed by Humboldt University and its partners, Flair is a multipurpose NLP library which can perform tasks such as name entity recognition (NER), part-of-speech taging (PoS), load word embeddings (from GloVe, BERT, ELMo) and incorporate it into PyTorch for learning. You can see their [official page on GitHub here](https://github.com/flairNLP/flair)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKd_iXuD1ref","outputId":"a87e67a1-c3b3-44ad-e3c8-b17aace12f65"},"source":["!pip install flair\n","\n","from flair.models import TextClassifier\n","from flair.data import Sentence\n","from flair.data_fetcher import NLPTaskDataFetcher\n","from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentLSTMEmbeddings\n","from flair.trainers import ModelTrainer\n","from flair.data import Corpus\n","\n","from pathlib import Path\n","\n","import pandas as pd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting flair\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/3a/1b46a0220d6176b22bcb9336619d1731301bc2c75fa926a9ef953e6e4d58/flair-0.8.0.post1-py3-none-any.whl (284kB)\n","\u001b[K     |████████████████████████████████| 286kB 4.3MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n","Collecting langdetect\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n","\u001b[K     |████████████████████████████████| 983kB 35.2MB/s \n","\u001b[?25hCollecting bpemb>=0.3.2\n","  Downloading https://files.pythonhosted.org/packages/f2/6f/9191b85109772636a8f8accb122900c34db26c091d2793218aa94954524c/bpemb-0.3.3-py3-none-any.whl\n","Collecting konoha<5.0.0,>=4.0.0\n","  Downloading https://files.pythonhosted.org/packages/71/70/48a0bd55f79c328504fe6fe7ae8ff651f77a2aadbb1911701385d9bb5ca3/konoha-4.6.5-py3-none-any.whl\n","Collecting huggingface-hub\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n","Collecting deprecated>=1.2.4\n","  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n","Collecting gdown==3.12.2\n","  Downloading https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n","Collecting mpld3==0.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n","\u001b[K     |████████████████████████████████| 798kB 34.2MB/s \n","\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n","Collecting sentencepiece==0.1.95\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n","\u001b[K     |████████████████████████████████| 1.2MB 35.1MB/s \n","\u001b[?25hCollecting torch<=1.7.1,>=1.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n","\u001b[K     |████████████████████████████████| 776.8MB 23kB/s \n","\u001b[?25hCollecting segtok>=1.5.7\n","  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n","Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n","Collecting janome\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n","\u001b[K     |████████████████████████████████| 19.7MB 1.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n","Collecting transformers>=4.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/f7/9287c330b0a195f67b6ce92f471c94084cdec1f596eb13b374704831f15a/transformers-4.8.0-py3-none-any.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 25.9MB/s \n","\u001b[?25hCollecting ftfy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n","\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n","\u001b[?25hCollecting sqlitedict>=1.6.0\n","  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.4.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from langdetect->flair) (1.15.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (2.23.0)\n","Collecting importlib-metadata<4.0.0,>=3.7.0\n","  Downloading https://files.pythonhosted.org/packages/52/d0/bdb31463f2d9ca111e39b268518e9baa3542ef73ca449b711a7b4da69764/importlib_metadata-3.10.1-py3-none-any.whl\n","Collecting overrides<4.0.0,>=3.0.0\n","  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.0.12)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (20.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.7.4.3)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.1.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n","Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.4)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 32.3MB/s \n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 29.0MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (3.13)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.4.1)\n","Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n","Building wheels for collected packages: gdown\n","  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gdown: filename=gdown-3.12.2-cp37-none-any.whl size=9705 sha256=de7b705d1bd55870ac9643aae85a27033d786f2c9e0898b67c24c4c8deab1b8a\n","  Stored in directory: /root/.cache/pip/wheels/81/d0/d7/d9983facc6f2775411803e0e2d30ebf98efbf2fc6e57701e09\n","Successfully built gdown\n","Building wheels for collected packages: langdetect, mpld3, segtok, ftfy, sqlitedict, overrides\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993242 sha256=0c60df14bab132ea495de0b31b13cba53c67cbe5e3816cd7bb0a7d64372c27b1\n","  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n","  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for mpld3: filename=mpld3-0.3-cp37-none-any.whl size=116704 sha256=a7c5484055e2764d4593b9b9f4bf382548ba6cc6a484471bf9033c9fbabe9610\n","  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n","  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25031 sha256=abdfd26108c142273bc6f7122aa7a2762738b78be9ebfa62bcb8080ef87d3183\n","  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41935 sha256=473bfb7d6ce844d503380e47fb7334342cd8f5835c511f83ade6afad7fe043a9\n","  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n","  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp37-none-any.whl size=14393 sha256=da9b1dc182f9193d9b9913b920b2e234ee2aa8cfe919de6d9a9744be01b7e392\n","  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10187 sha256=2be2d333820a73d1ec8a527c14fad8923e937abe2fcc6c3b2e5695fbc02cf8a9\n","  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n","Successfully built langdetect mpld3 segtok ftfy sqlitedict overrides\n","\u001b[31mERROR: torchvision 0.10.0+cu102 has requirement torch==1.9.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.7.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: konoha 4.6.5 has requirement requests<3.0.0,>=2.25.1, but you'll have requests 2.23.0 which is incompatible.\u001b[0m\n","Installing collected packages: langdetect, sentencepiece, bpemb, importlib-metadata, overrides, konoha, huggingface-hub, deprecated, gdown, mpld3, torch, segtok, janome, sacremoses, tokenizers, transformers, ftfy, sqlitedict, flair\n","  Found existing installation: importlib-metadata 4.5.0\n","    Uninstalling importlib-metadata-4.5.0:\n","      Successfully uninstalled importlib-metadata-4.5.0\n","  Found existing installation: gdown 3.6.4\n","    Uninstalling gdown-3.6.4:\n","      Successfully uninstalled gdown-3.6.4\n","  Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","Successfully installed bpemb-0.3.3 deprecated-1.2.12 flair-0.8.0.post1 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.0.12 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 sacremoses-0.0.45 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.3 torch-1.7.1 transformers-4.8.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AHyBO9POV86l"},"source":["As an example, let's load the text classifier on English sentiment and try out our sample sentence. You can type out any sentence that you want. Did flair correctly identify the sentiment? "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MJUH0jiBBlMT","outputId":"d0a17693-66e9-4ef3-e0ef-710528c2c4f3"},"source":["classifier = TextClassifier.loadted every minute of it('en-sentiment')\n","\n","example_text = ['hated every minute of it']\n","sentence = Sentence(example_text)\n","\n","classifier.predict(sentence)\n","\n","print(f'The example sentence is: {sentence.labels}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-24 11:03:11,798 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n","The example sentence is: [NEGATIVE (1.0)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9BKtO2ALs9Rk"},"source":["In flair terminology, the corpus is the dataset that will be used to train the model. It is composed of 3 lists:  train sentences, dev (validation) sentences, and test sentences. You must split all of your data into these three sets before the model can begin training. Guidance for \n","\n","You can also download full datasets as well. For example, we can download the same dataset as used above by simply calling the following."]},{"cell_type":"code","metadata":{"id":"vcKLOmTCs5-H"},"source":["from flair.datasets import IMDB\n","\n","corpus: Corpus = IMDB()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R1j6JyI3tkzq"},"source":["Secondly, you must get the labels for the model so that they can be used later in training."]},{"cell_type":"code","metadata":{"id":"36ifrtystijv"},"source":["label_dict = corpus.make_label_dictionary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8y3eN2gt2nJ"},"source":["As mentioned earlier, word embeddings are immensely useful ways to express words in a numerical form for a computer to then use for algorithms. Flair allows many different embeddings for future training. This includes embeddings from GloVe, ELMo and BERT. We can just instantiate the embedding classfor now to be used in the next step. All embeddings are PyTorch vectors, so they can be immediately used for training and fine-tuning. Different options for [word embeddings are on this page](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_3_WORD_EMBEDDING.md). \n","\n"]},{"cell_type":"code","metadata":{"id":"sKil_rl_tqmi"},"source":["word_embeddings = [WordEmbeddings('glove')]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fU6rMhLv9WY"},"source":["We will need to aggregate word embeddings somehow to create a meaning from the document. This is called a document embedding and, like word embeddings, [there are a lot of options offered by the developers of flair as well](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_5_DOCUMENT_EMBEDDINGS.md). For RNN embeddings, the embedding dimensionality depends on the number of hidden states specified and whether the RNN is bidirectional or not. The documentation contains more specifics. Good to know as well ist that a GRU-type RNN is instantiated with this function."]},{"cell_type":"code","metadata":{"id":"dL8vMQm-tttU"},"source":["document_embeddings = DocumentRNNEmbeddings(word_embeddings, hidden_size=256)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ELzEnU2Zb4z5","outputId":"41e1c645-39c6-4d7b-fba8-7dbf72b1b7b3"},"source":["classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n","\n","trainer = ModelTrainer(classifier, corpus)\n","\n","trainer.train('resources/taggers/imdb',\n","              learning_rate=0.1,\n","              mini_batch_size=32,\n","              anneal_factor=0.5,\n","              patience=5,\n","              max_epochs=150)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-06-24 11:03:23,613 Reading data from .\n","2021-06-24 11:03:23,614 Train: testimdb.csv\n","2021-06-24 11:03:23,615 Dev: valimdb.csv\n","2021-06-24 11:03:23,616 Test: trainimdb.csv\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated function (or staticmethod) load_classification_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"},{"output_type":"stream","text":["2021-06-24 11:03:45,086 Reading data from /root/.flair/datasets/imdb_v2-rebalanced\n","2021-06-24 11:03:45,087 Train: /root/.flair/datasets/imdb_v2-rebalanced/train-all.txt\n","2021-06-24 11:03:45,088 Dev: None\n","2021-06-24 11:03:45,094 Test: None\n","2021-06-24 11:03:48,145 Computing label dictionary. Progress:\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 91000/91000 [10:39<00:00, 142.32it/s]"],"name":"stderr"},{"output_type":"stream","text":["2021-06-24 11:14:28,029 [b'POSITIVE', b'NEGATIVE']\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["2021-06-24 11:14:29,251 ----------------------------------------------------------------------------------------------------\n","2021-06-24 11:14:29,252 Model: \"TextClassifier(\n","  (document_embeddings): DocumentRNNEmbeddings(\n","    (embeddings): StackedEmbeddings(\n","      (list_embedding_0): WordEmbeddings('glove')\n","    )\n","    (word_reprojection_map): Linear(in_features=100, out_features=100, bias=True)\n","    (rnn): GRU(100, 256, batch_first=True)\n","    (dropout): Dropout(p=0.5, inplace=False)\n","  )\n","  (decoder): Linear(in_features=256, out_features=2, bias=True)\n","  (loss_function): CrossEntropyLoss()\n","  (beta): 1.0\n","  (weights): None\n","  (weight_tensor) None\n",")\"\n","2021-06-24 11:14:29,254 ----------------------------------------------------------------------------------------------------\n","2021-06-24 11:14:29,256 Corpus: \"Corpus: 81000 train + 9000 dev + 10000 test sentences\"\n","2021-06-24 11:14:29,259 ----------------------------------------------------------------------------------------------------\n","2021-06-24 11:14:29,262 Parameters:\n","2021-06-24 11:14:29,264  - learning_rate: \"0.1\"\n","2021-06-24 11:14:29,266  - mini_batch_size: \"32\"\n","2021-06-24 11:14:29,268  - patience: \"5\"\n","2021-06-24 11:14:29,269  - anneal_factor: \"0.5\"\n","2021-06-24 11:14:29,271  - max_epochs: \"150\"\n","2021-06-24 11:14:29,274  - shuffle: \"True\"\n","2021-06-24 11:14:29,276  - train_with_dev: \"False\"\n","2021-06-24 11:14:29,278  - batch_growth_annealing: \"False\"\n","2021-06-24 11:14:29,280 ----------------------------------------------------------------------------------------------------\n","2021-06-24 11:14:29,282 Model training base path: \"resources/taggers/imdb\"\n","2021-06-24 11:14:29,284 ----------------------------------------------------------------------------------------------------\n","2021-06-24 11:14:29,286 Device: cuda:0\n","2021-06-24 11:14:29,292 ----------------------------------------------------------------------------------------------------\n","2021-06-24 11:14:29,294 Embeddings storage mode: cpu\n","2021-06-24 11:14:29,299 ----------------------------------------------------------------------------------------------------\n","2021-06-24 11:17:28,809 epoch 1 - iter 253/2532 - loss 0.00710450 - samples/sec: 55.10 - lr: 0.100000\n","2021-06-24 11:20:21,436 epoch 1 - iter 506/2532 - loss 0.00387996 - samples/sec: 56.33 - lr: 0.100000\n","2021-06-24 11:23:12,166 epoch 1 - iter 759/2532 - loss 0.02312080 - samples/sec: 56.83 - lr: 0.100000\n","2021-06-24 11:26:04,440 epoch 1 - iter 1012/2532 - loss 0.01762236 - samples/sec: 56.66 - lr: 0.100000\n","2021-06-24 11:28:55,093 epoch 1 - iter 1265/2532 - loss 0.02215782 - samples/sec: 55.66 - lr: 0.100000\n","2021-06-24 11:31:51,816 epoch 1 - iter 1518/2532 - loss 0.02272633 - samples/sec: 54.74 - lr: 0.100000\n","2021-06-24 11:34:44,681 epoch 1 - iter 1771/2532 - loss 0.01960266 - samples/sec: 56.58 - lr: 0.100000\n","2021-06-24 11:37:36,584 epoch 1 - iter 2024/2532 - loss 0.02575564 - samples/sec: 55.87 - lr: 0.100000\n","2021-06-24 11:40:31,208 epoch 1 - iter 2277/2532 - loss 0.02303802 - samples/sec: 55.82 - lr: 0.100000\n","2021-06-24 11:43:20,839 epoch 1 - iter 2530/2532 - loss 0.02078760 - samples/sec: 56.29 - lr: 0.100000\n","2021-06-24 11:43:21,845 ----------------------------------------------------------------------------------------------------\n","2021-06-24 11:43:21,846 EPOCH 1 done: loss 0.0208 - lr 0.1000000\n","2021-06-24 11:47:15,303 DEV : loss 5.9060187339782715 - score 0.4993\n","2021-06-24 11:48:45,811 BAD EPOCHS (no improvement): 0\n","saving best model\n","2021-06-24 11:48:48,713 ----------------------------------------------------------------------------------------------------\n","2021-06-24 11:51:44,552 epoch 2 - iter 253/2532 - loss 0.71950738 - samples/sec: 55.93 - lr: 0.100000\n","2021-06-24 11:54:35,482 epoch 2 - iter 506/2532 - loss 0.70365664 - samples/sec: 55.97 - lr: 0.100000\n","2021-06-24 11:57:27,419 epoch 2 - iter 759/2532 - loss 0.69476061 - samples/sec: 55.81 - lr: 0.100000\n","2021-06-24 12:00:18,619 epoch 2 - iter 1012/2532 - loss 0.68933450 - samples/sec: 56.49 - lr: 0.100000\n","2021-06-24 12:03:09,867 epoch 2 - iter 1265/2532 - loss 0.68408521 - samples/sec: 56.36 - lr: 0.100000\n","2021-06-24 12:05:58,024 epoch 2 - iter 1518/2532 - loss 0.68045443 - samples/sec: 57.11 - lr: 0.100000\n","2021-06-24 12:08:45,018 epoch 2 - iter 1771/2532 - loss 0.67848042 - samples/sec: 57.53 - lr: 0.100000\n","2021-06-24 12:11:34,227 epoch 2 - iter 2024/2532 - loss 0.67528936 - samples/sec: 57.51 - lr: 0.100000\n","2021-06-24 12:14:25,751 epoch 2 - iter 2277/2532 - loss 0.67240182 - samples/sec: 55.50 - lr: 0.100000\n","2021-06-24 12:17:16,948 epoch 2 - iter 2530/2532 - loss 0.67003065 - samples/sec: 57.06 - lr: 0.100000\n","2021-06-24 12:17:17,826 ----------------------------------------------------------------------------------------------------\n","2021-06-24 12:17:17,830 EPOCH 2 done: loss 0.6700 - lr 0.1000000\n","2021-06-24 12:21:07,872 DEV : loss 0.6880109310150146 - score 0.5458\n","2021-06-24 12:22:35,490 BAD EPOCHS (no improvement): 0\n","saving best model\n","2021-06-24 12:22:38,168 ----------------------------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gqx4mOgGwoiQ"},"source":["sentence = \"I don't think it was the best movie ever but I liked it\"\n","classifier.predict(sentence)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TWBsdtKNUmFN"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","imdb = pd.read_csv('/content/drive/My Drive/IMDB Dataset.csv', header=0)\n","imdb[\"label\"] = (imdb[\"sentiment\"]==\"positive\").astype(int)\n","imdb.drop(\"sentiment\", axis=1, inplace=True)\n","train, val, test = np.split(imdb.sample(frac=1), [int(.6*len(imdb)), int(.8*len(imdb))])"],"execution_count":null,"outputs":[]}]}